{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee929345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install whisper -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc6d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyannote.audio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e3092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U openai-whisper -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d54f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335fc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scenedetect[opencv] opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55abeed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install moviepy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb694f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install imutils -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a322661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytesseract -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b7bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c832168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Если каких-то библиотек нет, необходимо доустановить :)\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import split_on_silence\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from moviepy.editor import VideoFileClip\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from ultralytics import YOLO, solutions\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "import whisper\n",
    "import torch\n",
    "import spacy\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e05557df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in input_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Building video input_video.mp4.\n",
      "Moviepy - Writing video input_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready input_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Разделение видео и аудио\n",
    "def to_audio_and_clip(path:str) -> None:\n",
    "    input_video = VideoFileClip(path)\n",
    "    input_video.audio.write_audiofile('input_audio.mp3')\n",
    "    input_video.without_audio().write_videofile('input_video.mp4', codec='libx264')\n",
    "    \n",
    "to_audio_and_clip(r'C:\\Users\\frase\\OneDrive\\Рабочий стол\\test_text2_30sec.mp4') # Путь к файлу для разметки видео"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806756d",
   "metadata": {},
   "source": [
    "# Аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8260e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'small' # На gpu можно выбрать модель base и выше\n",
    "model = whisper.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee22335",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'input_audio.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143d1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35aafdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датасета под выходные данные модели whisper (если понадобятся дополнительные данные, то можно не удалять столбцы)\n",
    "df = pd.DataFrame(result['segments']).drop(['id', 'tokens', 'temperature', 'no_speech_prob', 'compression_ratio', 'avg_logprob', 'seek'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f91100",
   "metadata": {},
   "source": [
    "## Классификация звуков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bedea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка аудиофайла\n",
    "def preprocess_audio(filename, target_sr=16000):\n",
    "    y, sr = librosa.load(filename, sr=None)  # Загружаем с оригинальной частотой\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)  # Изменяем частоту\n",
    "    y = y / np.max(np.abs(y))  # Нормализация\n",
    "    return y, target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f9fe9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификация звука в аудиофрагментах\n",
    "def classification_audio(audio:list, threshold = 0.5):\n",
    "    model_url = \"https://tfhub.dev/google/yamnet/1\"\n",
    "    model = hub.load(model_url)\n",
    "    \n",
    "    result_classes = []\n",
    "    for i in range(len(audio)):\n",
    "        filename = audio[i]\n",
    "        y, sr = preprocess_audio(filename)\n",
    "        \n",
    "        scores, embeddings, spectrogram = model(y)\n",
    "        \n",
    "        # Определение классов звуковых событий\n",
    "        class_map_path = model.class_map_path().numpy().decode('utf-8')\n",
    "        class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
    "        \n",
    "        top_k = 5 # Кол-во наиболее вероятных классов\n",
    "        \n",
    "        scores = scores.numpy() if tf.is_tensor(scores) else scores  # Преобразуем в numpy, если необходимо\n",
    "        \n",
    "        # Проверка на размерность\n",
    "        if len(scores.shape) > 1:\n",
    "            scores = scores[0]\n",
    "        \n",
    "        top_k_indices = np.argsort(scores)[-top_k:][::-1] # Сортируем и отбираем классы\n",
    "        \n",
    "        classes = [class_names[top_k_indices[0]]] # Первый обязательно добавляем в выбранные классы\n",
    "        \n",
    "        # Отбираем дополнительные возможные классы\n",
    "        for i in range(1, len(top_k_indices)):\n",
    "            if scores[top_k_indices[i]] > threshold:\n",
    "                classes.append(class_names[top_k_indices[i]])\n",
    "                \n",
    "        result_classes.append((filename, classes))\n",
    "    \n",
    "    return result_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36089797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбивка всего аудио на фрагменты\n",
    "def f(row):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    start_time = row['start']*1000  # Начало обрезки в миллисекундах\n",
    "    end_time = row['end']*1000  # Конец обрезки в миллисекундах\n",
    "    if end_time - start_time <= 1000:\n",
    "        return 'underfind'\n",
    "    \n",
    "    trimmed_audio = audio[start_time:end_time]\n",
    "    trimmed_audio.export(\"output.mp3\", format=\"mp3\")\n",
    "    ans = classification_audio([\"output.mp3\"])\n",
    "    return ';'.join(ans[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed87be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['classifier_result'] = df.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057a5e0",
   "metadata": {},
   "source": [
    "## Определение спикеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488c971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:D:\\Programs\\anaconda3\\Lib\\inspect.py:992: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "\n",
      "WARNING:py.warnings:D:\\Programs\\anaconda3\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1808.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n",
      "WARNING:py.warnings:D:\\Programs\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:D:\\Programs\\anaconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_for_speakers = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token=\"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\") # Токен одного из участников команды\n",
    "\n",
    "diarization = pipeline_for_speakers(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb13e1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_speakers = pd.DataFrame(columns=['start', 'stop', 'speaker'])\n",
    "\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    df_speakers.loc[len(df_speakers)] = [turn.start, turn.end, speaker.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9beef079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030969</td>\n",
       "      <td>11.624094</td>\n",
       "      <td>speaker_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.979719</td>\n",
       "      <td>4.030344</td>\n",
       "      <td>speaker_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      start       stop     speaker\n",
       "0  0.030969  11.624094  speaker_00\n",
       "1  3.979719   4.030344  speaker_01"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16a678da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируется отдельно, потому что сложно совместить с основным датасетом по аудио, из-за времени\n",
    "speakers_json = df_speakers.to_json(force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524df9fb",
   "metadata": {},
   "source": [
    "## Выделение смысла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64750621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f14f46ea732433f8f70a52ae449e869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_meaning = SentenceTransformer('sberbank-ai/sbert_large_nlu_ru')\n",
    "\n",
    "embeddings = model_meaning.encode(df['text'], batch_size=32, show_progress_bar=True)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.4, min_samples=2, metric='cosine')\n",
    "labels = dbscan.fit_predict(embeddings)\n",
    "res = []\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    res.append(label)\n",
    "\n",
    "df['group meaning'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a24102f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95872285a552490e8bf4e5ce47f4fde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(50364, 1536)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50364, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 24)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50364, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 24)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=50364, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('RussianNLP/FRED-T5-Summarizer',eos_token='</s>')\n",
    "model_meaning = T5ForConditionalGeneration.from_pretrained('RussianNLP/FRED-T5-Summarizer')\n",
    "device = 'cpu' # На gpu исполльзовать 'cuda'\n",
    "model_meaning.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e90d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definition_of_meaning(text):\n",
    "    input_text = '''<LM> Напиши очень кратко одним предложением (сократи текст по смыслу), о чем говорится в тексте. Нельзя использовать слово \"текст\" в ответе.\\n''' + text\n",
    "    input_ids = torch.tensor([tokenizer.encode(input_text)]).to(device)\n",
    "    outputs = model_meaning.generate(input_ids,eos_token_id = tokenizer.eos_token_id,\n",
    "                        num_beams = 5,\n",
    "                        min_new_tokens = 17,\n",
    "                        max_new_tokens = 200,\n",
    "                        do_sample = True,\n",
    "                        no_repeat_ngram_size = 4,\n",
    "                        top_p = 0.9)\n",
    "    return tokenizer.decode(outputs[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a96c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_meaning = []\n",
    "\n",
    "step = 3 # Сколько подряд отрывков аудио брать для выделения в них смысла\n",
    "for i in range(0, len(df['text']) - step, step):\n",
    "    text = ''.join(list(df['text'].loc[i:i+step]))\n",
    "    res_sum = str(definition_of_meaning(text))[:-4]\n",
    "    for j in range(step):\n",
    "        text_meaning.append(res_sum)\n",
    "        \n",
    "k = len(df['text']) - len(text_meaning)\n",
    "text = ''.join(list(df['text'].loc[len(text_meaning):]))\n",
    "res_sum = str(definition_of_meaning(text))[:-4]\n",
    "\n",
    "for i in range(k):\n",
    "    text_meaning.append(res_sum)\n",
    "\n",
    "df['meaning'] = text_meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf25d6",
   "metadata": {},
   "source": [
    "## Проверка на стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2b36834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтения стоп слов из файла bad_word.txt\n",
    "def read_bad_words(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            bad_words = {line.strip().lower() for line in file if line.strip()}\n",
    "        return bad_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл '{file_path}' не найден.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e1df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка наличия стоп слов в предложениях\n",
    "def check_bad_words_in_sentence(sentence, bad_words, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if lemma in bad_words:\n",
    "            return True, token.text, lemma\n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "bad_words_file = 'static/bad_words.txt'\n",
    "bad_words = read_bad_words(bad_words_file)\n",
    "sentences = list(df['text'])\n",
    "bad_lines = []\n",
    "\n",
    "for idx, sentence in enumerate(sentences, start=1):\n",
    "    has_bad_word, word, lemma = check_bad_words_in_sentence(sentence, bad_words, nlp)\n",
    "    if has_bad_word:\n",
    "        bad_lines.append(idx)\n",
    "\n",
    "id_stop_words = np.array(['no'] * len(df))\n",
    "id_stop_words[bad_lines] = 'yes'\n",
    "df['stop words'] = id_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56691765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>classifier_result</th>\n",
       "      <th>group meaning</th>\n",
       "      <th>meaning</th>\n",
       "      <th>stop words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Все новенькие только на нашем сайте. Сканируй...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>-1</td>\n",
       "      <td>На нашем сайте вы можете скачать караоке минус...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start   end                                               text  \\\n",
       "0    0.0  12.0   Все новенькие только на нашем сайте. Сканируй...   \n",
       "\n",
       "  classifier_result  group meaning  \\\n",
       "0            Speech             -1   \n",
       "\n",
       "                                             meaning stop words  \n",
       "0  На нашем сайте вы можете скачать караоке минус...         no  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e718186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df.to_json(force_ascii=False) # Конечный датасет с аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e28ba",
   "metadata": {},
   "source": [
    "# Видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39637719",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'input_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cd16c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смена сцен\n",
    "def scene_detector(path:str) -> list:\n",
    "    video = open_video(path)\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector())\n",
    "    scene_manager.detect_scenes(video)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    return scene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df0267de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка слов\n",
    "def sort_words_by_position_with_tolerance(boxes, words, tolerance=10):\n",
    "    combined = list(zip(boxes, words))\n",
    "    \n",
    "    combined_sorted = sorted(combined, key=lambda item: (round(item[0][1] / tolerance), item[0][0]))\n",
    "    \n",
    "    sorted_boxes = [item[0] for item in combined_sorted]\n",
    "    sorted_words = [item[1] for item in combined_sorted]\n",
    "    \n",
    "    return sorted_boxes, sorted_words\n",
    "\n",
    "# Предобработка картинки\n",
    "def preprocess_image(image, target_size):\n",
    "    orig = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    (origH, origW) = image.shape[:2]\n",
    "    rW = origW / float(target_size[0])\n",
    "    rH = origH / float(target_size[1])\n",
    "    \n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    \n",
    "    return resized_image, orig, rW, rH\n",
    "\n",
    "def postprocess(scores, geometry, min_confidence):\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    \n",
    "    for y in range(0, numRows):\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0, xData1, xData2, xData3 = geometry[0, 0:3 + 1, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "    \n",
    "        for x in range(0, numCols):\n",
    "            if scoresData[x] < min_confidence:\n",
    "                continue\n",
    "    \n",
    "            offsetX, offsetY = (x * 4.0, y * 4.0)\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            \n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "    \n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            \n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])    \n",
    "    return rects, confidences\n",
    "\n",
    "def extract_text_from_region(image, startX, startY, endX, endY):\n",
    "    # Обрезаем регион изображения\n",
    "    image = image[startY:endY, startX:endX]\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.medianBlur(image, 3)\n",
    "    border = 150\n",
    "    image[image[:][:] > border] = 255\n",
    "    image[image[:][:] <= border] = 0\n",
    "    \n",
    "    text = re.sub(r'[^а-яё]', '', pytesseract.image_to_string(image, config='--oem 1 --psm 8', lang='rus').lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def east_detect(image, net, target_size=(320, 192), min_confidence=0.6):\n",
    "    layerNames = [\n",
    "        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "        \"feature_fusion/concat_3\"\n",
    "    ]\n",
    "    \n",
    "    resized_image, orig, rW, rH = preprocess_image(image, target_size)\n",
    "    (H, W) = resized_image.shape[:2]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(resized_image, 1.0, (W, H),\n",
    "                                 (123.68, 116.78, 113.94), swapRB=False, crop=False)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    scores, geometry = net.forward(layerNames)\n",
    "    \n",
    "    rects, confidences = postprocess(scores, geometry, min_confidence)\n",
    "    \n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    padding = 8\n",
    "    text = []\n",
    "    boxes_r = []\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        startX = max(0, int(startX * rW) - padding)\n",
    "        startY = max(0, int(startY * rH) - padding)\n",
    "        endX = min(orig.shape[1], int(endX * rW) + padding)\n",
    "        endY = min(orig.shape[0], int(endY * rH) + padding)\n",
    "        \n",
    "        #cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 1)\n",
    "        text_tmp = extract_text_from_region(image, startX, startY, endX, endY)\n",
    "        if len(text_tmp) >= 3:\n",
    "            boxes_r.append([startX, startY, endX, endY])\n",
    "            text.append(text_tmp)\n",
    "        \n",
    "    \n",
    "    boxes_r, text  = sort_words_by_position_with_tolerance(boxes_r, text, tolerance=30)\n",
    "    print(f\"Time taken: {time.time() - start:.4f} seconds\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e226397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение текста с картинки\n",
    "def get_text_for_video(path:str, frame_step=60) -> list:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    assert cap.isOpened()\n",
    "    results = []\n",
    "    frame_count = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    net = cv2.dnn.readNet(\"model/frozen_east_text_detection.pb\")\n",
    "    for frame_id in range(0, total_frames, frame_step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = cap.read()\n",
    "        print(f\"Обрабатываем кадр {frame_id + 1} ({(frame_id // frame_step) + 1})\", end='    ')\n",
    "        results.append(east_detect(frame, net, (32 * 24, 32 * 14), 0.7))\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return results, frame_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd091458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатываем кадр 1 (1)    Time taken: 1.9242 seconds\n",
      "Обрабатываем кадр 61 (2)    Time taken: 1.9743 seconds\n",
      "Обрабатываем кадр 121 (3)    Time taken: 1.2322 seconds\n",
      "Обрабатываем кадр 181 (4)    Time taken: 2.2041 seconds\n",
      "Обрабатываем кадр 241 (5)    Time taken: 1.4320 seconds\n",
      "Обрабатываем кадр 301 (6)    Time taken: 2.0116 seconds\n",
      "Обрабатываем кадр 361 (7)    Time taken: 1.6891 seconds\n",
      "Обрабатываем кадр 421 (8)    Time taken: 1.5120 seconds\n",
      "Обрабатываем кадр 481 (9)    Time taken: 1.6615 seconds\n",
      "Обрабатываем кадр 541 (10)    Time taken: 1.8275 seconds\n",
      "Обрабатываем кадр 601 (11)    Time taken: 1.6457 seconds\n",
      "Обрабатываем кадр 661 (12)    Time taken: 1.6693 seconds\n",
      "Обрабатываем кадр 721 (13)    Time taken: 2.4677 seconds\n",
      "Обрабатываем кадр 781 (14)    Time taken: 2.4949 seconds\n",
      "Обрабатываем кадр 841 (15)    Time taken: 2.5755 seconds\n"
     ]
    }
   ],
   "source": [
    "results = get_text_for_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b2bc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка слов с видео на стоп слова\n",
    "stop_words_in_video = pd.DataFrame(columns=['framerate', 'stop words'])\n",
    "\n",
    "for i in range(len(results[0])):\n",
    "    if len(results[0][i]) > 0:\n",
    "        sentences = results[0][i]\n",
    "        bad_lines = []\n",
    "\n",
    "        for idx, sentence in enumerate(sentences, start=1):\n",
    "            has_bad_word, word, lemma = check_bad_words_in_sentence(sentence, bad_words, nlp)\n",
    "            if has_bad_word:\n",
    "                bad_lines.append(idx)\n",
    "        \n",
    "        if len(bad_lines) > 0:\n",
    "            stop_words_in_video.loc[len(stop_words_in_video)] = [i * results[1] + 1, 'yes']\n",
    "        else:\n",
    "            stop_words_in_video.loc[len(stop_words_in_video)] = [i * results[1] + 1, 'no']\n",
    "    else:\n",
    "        stop_words_in_video.loc[len(stop_words_in_video)] = [i * results[1] + 1, 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e03317a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка на nsfw контент\n",
    "def check_nsfw_on_video(path, frame_step=60):\n",
    "    classifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    assert cap.isOpened()\n",
    "\n",
    "    frame_count = 0\n",
    "    result = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    for frame_id in range(0, total_frames, frame_step):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        verdict = classifier(frame)\n",
    "        if verdict[0]['label'] == 'nsfw':\n",
    "            result.append(verdict[0]['score'])\n",
    "        else:\n",
    "            result.append(verdict[1]['score'])\n",
    "        \n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9e76bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация тепловых карт\n",
    "def heat_map(path:str) -> None:\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "    video_writer = cv2.VideoWriter(\"heatmap_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    heatmap_obj = solutions.Heatmap(\n",
    "        colormap=cv2.COLORMAP_PARULA,\n",
    "        view_img=False,\n",
    "        shape=\"circle\",\n",
    "        names=model.names,\n",
    "    )\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, im0 = cap.read()\n",
    "        if not success:\n",
    "            print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "            break\n",
    "\n",
    "        tracks = model.track(im0, persist=True, show=False)\n",
    "\n",
    "        im0 = heatmap_obj.generate_heatmap(im0, tracks)\n",
    "\n",
    "        video_writer.write(im0)\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d07b70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обнаружение объектов\n",
    "def obj_detection(path) -> None:\n",
    "    track_history = defaultdict(lambda: [])\n",
    "\n",
    "    model = YOLO(\"yolov8n-seg.pt\")  # segmentation model\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "    out = cv2.VideoWriter(\"instance-segmentation-object-tracking.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "    while True:\n",
    "        ret, im0 = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "            break\n",
    "\n",
    "        annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "        results = model.track(im0, persist=True)\n",
    "\n",
    "        if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "            masks = results[0].masks.xy\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "            for mask, track_id in zip(masks, track_ids):\n",
    "                color = colors(int(track_id), True)\n",
    "                txt_color = annotator.get_txt_color(color)\n",
    "                annotator.seg_bbox(mask=mask, mask_color=color, label=str(track_id), txt_color=txt_color)\n",
    "\n",
    "        out.write(im0)\n",
    "\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcfc55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nswf = check_nsfw_on_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "744a52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nswf = ['%.2f' % elem for elem in nswf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08cdbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nswf_in_video = pd.DataFrame({'framerate': [i + 1 for i in range(0, len(nswf) * 60, 60)], 'nswf': nswf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "991c5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = pd.merge(nswf_in_video, stop_words_in_video, on='framerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb50be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>framerate</th>\n",
       "      <th>nswf</th>\n",
       "      <th>stop words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121</td>\n",
       "      <td>0.03</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181</td>\n",
       "      <td>0.02</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241</td>\n",
       "      <td>0.02</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   framerate  nswf stop words\n",
       "0          1  0.13         no\n",
       "1         61  0.03         no\n",
       "2        121  0.03         no\n",
       "3        181  0.02         no\n",
       "4        241  0.02         no"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb92d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_json = video.to_json(force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42a73244",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_json = {'audio': df_json, 'spekears': speakers_json, 'video': video_json}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "741d9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(res_json, ensure_ascii=False)\n",
    " \n",
    "with open('result.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
